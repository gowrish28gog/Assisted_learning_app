import os
from dotenv import load_dotenv
from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer
import ollama
import streamlit as st

def load_huggingface_model(prompt: str, document: str, model_name: str = "llama3.2"):
    """
    Loads a Hugging Face model for text classification and uses it to classify the provided document based on the prompt.

    Args:
        prompt (str): The prompt/question to guide the model in classifying the document.
        document (str): The document to be classified by the model.
        model_name (str): The name of the Hugging Face model to use (default is 'llama3.2').

    Returns:
        result (list): The classification result returned by the model pipeline.
    
    Raises:
        ValueError: If the Hugging Face token is not found in the environment variables.
        ValueError: If the input text (prompt + document) exceeds the model's input token limit.
        ValueError: If the output generated by the model exceeds the model's output token limit.

    Example:
        result = load_huggingface_model("What is the sentiment?", "This is a positive document.")
    """

    # Load environment variables from .env file
    load_dotenv()
    
    # Get the Hugging Face token from environment variables
    hf_token = os.getenv('HUGGINGFACE_TOKEN')
    
    if not hf_token:
        raise ValueError("Hugging Face token not found in environment variables.")
    
    # Load the model and tokenizer
    model = AutoModelForSequenceClassification.from_pretrained(model_name, use_auth_token=hf_token)
    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=hf_token)
    
    # Check token length of the input (prompt + document)
    input_text = f"Document: {document}\n\nPrompt: {prompt}"
    input_tokens = tokenizer.encode(input_text, truncation=False)
    
    max_input_tokens = tokenizer.model_max_length
    
    if len(input_tokens) > max_input_tokens:
        raise ValueError(f"Input text exceeds the model's maximum input token length of {max_input_tokens} tokens.")
    
    # Create a pipeline
    nlp_pipeline = pipeline('text-classification', model=model, tokenizer=tokenizer)
    
    try:
        # Run the pipeline with the provided prompt
        result = nlp_pipeline(input_text)
    except Exception as e:
        st.error(f"An error occurred while generating the output: {str(e)}")
        raise e
    
    # Check the length of the output tokens
    output_text = result[0]['label']
    output_tokens = tokenizer.encode(output_text, truncation=False)
    
    if len(output_tokens) > max_input_tokens:
        raise ValueError(f"The generated output exceeds the model's output token limit of {max_input_tokens} tokens.")
    
    return result
